{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Installing and Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T03:46:44.535946Z",
     "iopub.status.busy": "2024-12-23T03:46:44.535623Z",
     "iopub.status.idle": "2024-12-23T03:47:21.012804Z",
     "shell.execute_reply": "2024-12-23T03:47:21.011839Z",
     "shell.execute_reply.started": "2024-12-23T03:46:44.535918Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# installing package\n",
    "!pip install -q galore-torch\n",
    "!pip install -q git+https://github.com/jiaweizzhao/GaLore\n",
    "!pip install -q accelerate\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q trl\n",
    "!pip install -q peft\n",
    "!pip install -q evaluate\n",
    "!pip install -U -q transformers\n",
    "!pip install -U -q datasets\n",
    "!pip install -U -q flash-attn\n",
    "!pip install -U -q deepspeed\n",
    "# %%capture\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T03:47:21.014412Z",
     "iopub.status.busy": "2024-12-23T03:47:21.014086Z",
     "iopub.status.idle": "2024-12-23T03:47:29.584978Z",
     "shell.execute_reply": "2024-12-23T03:47:29.584333Z",
     "shell.execute_reply.started": "2024-12-23T03:47:21.014378Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-23 03:47:28,528] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import accelerate\n",
    "from trl import SFTTrainer, setup_chat_format, DataCollatorForCompletionOnlyLM, apply_chat_template\n",
    "# from unsloth.models import FastLlamaModel # for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Checking the Tokenizer that support persian language or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T03:47:29.586833Z",
     "iopub.status.busy": "2024-12-23T03:47:29.586521Z",
     "iopub.status.idle": "2024-12-23T03:47:30.155677Z",
     "shell.execute_reply": "2024-12-23T03:47:30.154745Z",
     "shell.execute_reply.started": "2024-12-23T03:47:29.586800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "هر چی شما بگید، تغییرش بدم، یادگاری، دلقک، صبر، فشار هر چی شما بگید، تغییرش بدم، یادگاری، دلقک، صبر، فشار\n",
      "yep\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "token = \"\"\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"[PAD]\"})\n",
    "\n",
    "text = \"هر چی شما بگید، تغییرش بدم، یادگاری، دلقک، صبر، فشار\"\n",
    "\n",
    "encoded = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "decoded = tokenizer.decode(encoded, add_special_tokens=False)\n",
    "\n",
    "print(decoded, text)\n",
    "\n",
    "if decoded == text:\n",
    "    print(\"yep\")\n",
    "else:\n",
    "    print(\"uh hell nuh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"[PAD]\"})\n",
    "\n",
    "text = \"هر چی شما بگید، تغییرش بدم، یادگاری، دلقک، صبر، فشار\"\n",
    "\n",
    "encoded = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "decoded = tokenizer.decode(encoded, add_special_tokens=True)\n",
    "\n",
    "print(decoded, text)\n",
    "\n",
    "if decoded == text:\n",
    "    print(\"yep\")\n",
    "else:\n",
    "    print(\"uh hell nuh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"[PAD]\"})\n",
    "\n",
    "text = \"هر چی شما بگید، تغییرش بدم، یادگاری، دلقک، صبر، فشار\"\n",
    "\n",
    "encoded = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "decoded = tokenizer.decode(encoded, add_special_tokens=True)\n",
    "\n",
    "print(decoded, text)\n",
    "\n",
    "if decoded == text:\n",
    "    print(\"yep\")\n",
    "else:\n",
    "    print(\"uh hell nuh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Creating Dataset and Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T03:47:30.157185Z",
     "iopub.status.busy": "2024-12-23T03:47:30.156844Z",
     "iopub.status.idle": "2024-12-23T03:47:30.272719Z",
     "shell.execute_reply": "2024-12-23T03:47:30.271971Z",
     "shell.execute_reply.started": "2024-12-23T03:47:30.157153Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train dataset is: 12795\n",
      "The length of valid dataset is: 799\n",
      "The length of test dataset is: 2400\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Path to the Parquet file\n",
    "parquet_path = \"/kaggle/input/persian-slm-dataset/output.parquet\"\n",
    "\n",
    "# Load the Parquet file as a Hugging Face Dataset\n",
    "dataset = load_dataset(\"parquet\", data_files=parquet_path)[\"train\"]\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Compute split sizes\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = int(0.05 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "valid_dataset = dataset.select(range(train_size, train_size + valid_size))\n",
    "test_dataset = dataset.select(range(train_size + valid_size, len(dataset)))\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"valid\": valid_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# Display dataset statistics\n",
    "print(f\"The length of train dataset is: {len(dataset_dict['train'])}\")\n",
    "print(f\"The length of valid dataset is: {len(dataset_dict['valid'])}\")\n",
    "print(f\"The length of test dataset is: {len(dataset_dict['test'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T03:45:38.529741Z",
     "iopub.status.busy": "2024-12-23T03:45:38.529514Z",
     "iopub.status.idle": "2024-12-23T03:45:39.340755Z",
     "shell.execute_reply": "2024-12-23T03:45:39.340002Z",
     "shell.execute_reply.started": "2024-12-23T03:45:38.529718Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 12795\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 799\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['messages'],\n",
      "        num_rows: 2400\n",
      "    })\n",
      "})\n",
      "<class 'list'> [{'content': 'این مدل باید زبان فارسی را یاد بگیرد، شامل ساختار جملات، واژگان، و قواعد گرامری. هدف این یادگیری، درک صحیح زبان فارسی است، نه صرفاً پردازش اطلاعات یا روابط بین کلمات. به نحوی تمرین کنید که مدل بتواند زبان فارسی را به طور طبیعی و قابل درک تولید کند.\\n\\nبه سوال زیر بر اساس متن دانشت توضیح بده (به فارسی یا انگلیسی) پاسخ دهید. از اطلاعات دقیق و مرتبط با متن استفاده کنید تا پاسخ صحیح و مناسبی ارائه دهید.', 'role': 'system'}, {'content': 'اولین بار روزف اسپدین از سیمان چه استفادهای کرد', 'role': 'user'}, {'content': 'برای ساخت فانوس دریایی', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dict)\n",
    "print(type(dataset_dict[\"train\"][\"messages\"]), dataset_dict[\"train\"][\"messages\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# compute_dtype = getattr(torch, \"float16\")\n",
    "# quan_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_quant_type='nf4',\n",
    "#         bnb_4bit_compute_dtype=compute_dtype,\n",
    "#         bnb_4bit_use_double_quant=False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T03:47:33.962848Z",
     "iopub.status.busy": "2024-12-23T03:47:33.962517Z",
     "iopub.status.idle": "2024-12-23T03:47:38.765250Z",
     "shell.execute_reply": "2024-12-23T03:47:38.764549Z",
     "shell.execute_reply.started": "2024-12-23T03:47:33.962823Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "# modelpath_phi = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "# modelpath_llama_3 = \"meta_llama/Llama-3.2-3B\"\n",
    "modelpath_llama_1 = \"meta-llama/Llama-3.2-1B\"\n",
    "device = {\"\": 0}\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelpath_llama_1,    \n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map=device,\n",
    "    use_cache = False,\n",
    "    token=token,\n",
    "    # attn_implementation=\"flash_attention_2\", # For A100 GPU or Ampere GPU\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath_llama_1, use_fast = False, token=token)\n",
    "\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "if tokenizer.pad_token is None: \n",
    "    tokenizer.pad_token = \"[PAD]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pad and eos token should not be the same but in some model like llama are the same\n",
    "\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"Pad token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "print(f\"Eos token: {tokenizer.eos_token}\")\n",
    "print(f\"Eos token ID: {tokenizer.eos_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T03:47:40.506821Z",
     "iopub.status.busy": "2024-12-23T03:47:40.506537Z",
     "iopub.status.idle": "2024-12-23T03:47:42.869277Z",
     "shell.execute_reply": "2024-12-23T03:47:42.868363Z",
     "shell.execute_reply.started": "2024-12-23T03:47:40.506801Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: <|im_pad|>\n",
      "Pad token ID: 128258\n",
      "Eos token: <|im_end|>\n",
      "Eos token ID: 128257\n"
     ]
    }
   ],
   "source": [
    "# changing or adding pad token to tokenizer\n",
    "if tokenizer.pad_token is None or tokenizer.pad_token == tokenizer.eos_token:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<|im_pad|>\"})\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"<|im_pad|>\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"Pad token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"Eos token: {tokenizer.eos_token}\")\n",
    "print(f\"Eos token ID: {tokenizer.eos_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# verification of pad token\n",
    "input_text = [\n",
    "    \"hello how are you\",\n",
    "    \"Hello Thanks\"\n",
    "]\n",
    "encoded = tokenizer(input_text, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Input IDs: {encoded['input_ids']}\")\n",
    "print(f\"Attention Mask: {encoded['attention_mask']}\")\n",
    "print(f\"Input IDs shape: {encoded['input_ids'].shape}\")\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# checking the template\n",
    "dataset = {\n",
    "    \"prompt\": [[{\"role\": \"user\", \"content\": \"What color is the sky?\"}],\n",
    "               [{\"role\": \"user\", \"content\": \"Where is the sun?\"}]],\n",
    "    \"completion\": [[{\"role\": \"assistant\", \"content\": \"It is blue.\"}],\n",
    "                   [{\"role\": \"assistant\", \"content\": \"In the sky.\"}]]\n",
    "}\n",
    "\n",
    "dataset_sample = Dataset.from_dict(dataset)\n",
    "dataset_sample = dataset_sample.map(apply_chat_template, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "dataset_sample[\"prompt\"], dataset_sample[\"completion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "example = dataset_dict[\"train\"][0]\n",
    "apply_chat_template(example, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# checking data collator\n",
    "\n",
    "# Get a single sample from the dataset\n",
    "sample_data = dataset_dict[\"train\"][\"messages\"][0]\n",
    "\n",
    "# Define the instruction template\n",
    "instruction_template = \"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
    "\n",
    "# Format the sequence using the sample\n",
    "formatted_sequence = \"\".join(\n",
    "    instruction_template.format(role=msg[\"role\"], content=msg[\"content\"]) for msg in sample_data\n",
    ")\n",
    "\n",
    "# Tokenize the single sample\n",
    "tokenized_sample = tokenizer(formatted_sequence, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "# Convert to collator-compatible input by wrapping in a batch format\n",
    "tokenized_batch = {\n",
    "    \"input_ids\": tokenized_sample[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_sample[\"attention_mask\"]\n",
    "}\n",
    "\n",
    "# Pass the tokenized batch to the collator\n",
    "collated_batch = data_collator([tokenized_batch])\n",
    "\n",
    "# Extract collated data\n",
    "input_ids = collated_batch[\"input_ids\"]\n",
    "labels = collated_batch[\"labels\"]\n",
    "attention_mask = collated_batch[\"attention_mask\"]\n",
    "\n",
    "# Decode and print results for debugging\n",
    "decoded_inputs = tokenizer.batch_decode(input_ids, skip_special_tokens=False)\n",
    "decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=False)\n",
    "\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Decoded Inputs:\", decoded_inputs)\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Decoded Labels:\", decoded_labels)\n",
    "print(\"Attention Mask:\", attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T03:47:52.215789Z",
     "iopub.status.busy": "2024-12-23T03:47:52.215485Z",
     "iopub.status.idle": "2024-12-23T03:47:52.221076Z",
     "shell.execute_reply": "2024-12-23T03:47:52.219964Z",
     "shell.execute_reply.started": "2024-12-23T03:47:52.215768Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "# For finding seq_length of tokenizer\n",
    "max_seq_length = min(tokenizer.model_max_length, 1024)\n",
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# parallelism training\n",
    "def create_deepspeed_config(file_path=\"ds_config.json\"):\n",
    "    \"\"\"Creates a DeepSpeed configuration file with combined parallelism.\"\"\"\n",
    "\n",
    "    config = {\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_param\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True\n",
    "            },\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True\n",
    "            },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"train_micro_batch_size_per_gpu\": 2,\n",
    "        \"fp16\": {\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        \"pipeline\": {\n",
    "            \"enabled\": True,\n",
    "            \"partitions\": 2\n",
    "        },\n",
    "        \"tensor_parallel\": {\n",
    "            \"enabled\": True,\n",
    "            \"size\": 2\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Write configuration to JSON file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(config, json_file, indent=4)\n",
    "\n",
    "    print(f\"DeepSpeed configuration saved to {file_path}\")\n",
    "\n",
    "# Call the function to create the DeepSpeed config\n",
    "create_deepspeed_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T03:47:58.879412Z",
     "iopub.status.busy": "2024-12-23T03:47:58.879062Z",
     "iopub.status.idle": "2024-12-23T03:47:58.913798Z",
     "shell.execute_reply": "2024-12-23T03:47:58.912904Z",
     "shell.execute_reply.started": "2024-12-23T03:47:58.879384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# GaLore hyperparameters\n",
    "rank = 1024\n",
    "update_proj_gap = 200\n",
    "scale = 2\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = f\"/kaggle/working/slm_trian\",\n",
    "    logging_dir= f\"/kaggle/working/logs\",\n",
    "    eval_strategy = \"steps\",\n",
    "    weight_decay=0.0001,\n",
    "    label_names = [\"labels\"],\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 2,\n",
    "    # neftune_noise_alpha=5, # read about that\n",
    "    # gradient_accumulation_steps=4, #can't be used for galore\n",
    "    save_steps = 8000,\n",
    "    eval_steps = 500,\n",
    "    do_eval=True,\n",
    "    logging_steps = 1, \n",
    "    learning_rate = 1e-5,\n",
    "    num_train_epochs = 3,\n",
    "    # deepspeed = \"/kaggle/working/ds_config.json\", #can't be used for Galore\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    gradient_checkpointing = True,\n",
    "    # fp16=True,\n",
    "    # gradient_checkpointing_kwargs={'use_reentrant':False}, #read about that \n",
    "    optim = \"galore_adamw_8bit_layerwise\",\n",
    "    optim_target_modules = [\"attn\", \"mlp\"],\n",
    "    optim_args = f\"rank={rank}, update_proj_gap={update_proj_gap}, scale={scale}\",\n",
    "    report_to = \"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-23T03:59:23.865Z",
     "iopub.execute_input": "2024-12-23T03:48:02.093919Z",
     "iopub.status.busy": "2024-12-23T03:48:02.093642Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-ad9b36b422af>:3: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "Activated GaLoRE fine-tuning, depending on your model size and hardware, the training might take a while before starting. Please be patient !\n",
      "model.layers.0.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.0.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.0.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.0.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.1.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.1.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.1.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.1.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.2.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.2.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.2.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.2.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.3.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.3.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.3.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.3.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.4.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.4.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.4.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.4.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.5.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.5.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.5.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.5.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.6.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.6.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.6.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.6.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.7.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.7.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.7.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.7.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.8.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.8.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.8.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.8.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.9.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.9.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.9.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.9.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.10.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.10.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.10.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.10.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.11.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.11.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.11.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.11.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.12.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.12.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.12.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.12.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.13.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.13.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.13.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.13.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.14.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.14.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.14.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.14.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.15.self_attn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.15.self_attn.rotary_emb has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.15.mlp has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "model.layers.15.mlp.act_fn has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset_dict[\"train\"],\n",
    "    eval_dataset = dataset_dict['valid'],\n",
    "    data_collator = DataCollatorForCompletionOnlyLM(\n",
    "        instruction_template = \"<|im_start|>user\", \n",
    "        response_template = \"<|im_start|>assistant\", \n",
    "        tokenizer = tokenizer, \n",
    "        mlm = False),\n",
    "    # dataset_kwargs = dict(add_special_tokens = False),\n",
    "    args = training_arguments,\n",
    "    # dataset_text_field=\"message\"\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('/kaggle/working')\n",
    "tokenizer.save_pretrained('/kaggle/working')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6079256,
     "sourceId": 10013501,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
